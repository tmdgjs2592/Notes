**SVM Summary**
- Formulated SVM as a maximum margin classifier
- For non-separable data, slack variable is used to form the optimization
- Connected this to ERM $\rightarrow$ hinge loss
- Kernelized SVM through its dual & interpretted the solution

Ensemble Learning
Idea: use a set of predictions, eg. SVM, logistic regression, etc.
**Train multiple classifiers**
- Train multiple classifiers on a given training data and combine their predictions
- To combine predictions:
	- Majority vote: h=SIGN ($\overset{\sim}{h_1}+ \dots +\overset{\sim}{h_L}$), where $\overset{\sim}{h_l} \in \{+1,-1\}$
	-  Weighted majority vote: h=SIGN($w_1\overset{\sim}{h_1}+ \dots = w_L\overset{\sim}{h_L})$ where $\overset{\sim}{h_l} \in \{+1,-1\}$.
	-  Also can use mean, weighted mean, or median.

# Adaboost Algorithm
Weak classifier: $h = \underset{h\in \mathcal H}{arg min} \text{ }\epsilon_1(\overset{\sim}{h})$
Weighted error: $\epsilon_1(\overset{\sim}{h}) = \sum w_1(n) \cdot \mathbb{1}_{y_n \neq \overset{\sim}{h} (x_n)}$   
Weight update:
$$w_{t+1}(n) = \begin{cases} w_t(n) e^{-\beta_t} \text{ if } y_n = h_t(x_n)\\ w_t(n)e^{\beta_t} \text{ if }y_n \neq h_t(x_n) \end{cases}$$
$\rightarrow$ increase weight if misclassified and decrease weight if classified correctly.
$\rightarrow$ Giving more weights to the harder data points.

Final classifier:
$$h[x] = sign[\sum^T_{t=1} \beta_t h_t(x)]$$
Final classifier depends on winners of each iteration weighted by $\beta$.


