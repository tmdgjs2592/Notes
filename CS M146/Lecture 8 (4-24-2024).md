# Kernel methods
Q:
Can we get the benefit of operating in richer space while computation to find the model is comparable to original dimension D?

From regression model,
$$\hat{\theta} = (X^TX)^{-1}X^TY$$
To 
$$\hat{\theta_{new}} = (\phi^T\phi)^{-1}\phi^TY$$
where $\phi$ maps the original feature vector $x$ to a $M$-dimensional new feature vector.

**Ridge Regression**
$\nabla J(\theta) = 2\Phi^T\Phi \theta + 2\lambda \theta - 2\Phi^T y$
Solving for $\nabla J(\theta)$
$$
\begin{align}
&2\Phi^T\Phi \theta + 2\lambda \theta - 2\Phi^T y = 0 \\
& \lambda \theta = \Phi^T y -\Phi^T\Phi \theta \\
& \lambda \theta = \sum ^N_{i=1}[y_n -\theta^T\phi(x_n)]\phi(x_n) \\
& \theta = \sum ^N_{n=1}\frac{1}{\lambda}[y_n -\theta^T\phi(x_n)]\phi(x_n) \\
& \theta \in span \{\phi(x_1), ... \phi(x_N)\}
\end{align}
$$
Here, $\sum ^N_{n=1}\frac{1}{\lambda}[y_n -\theta^T\phi(x_n)]$ = $\alpha_n$ is a scalar.
Implication: If we know $\{\alpha_n\}^N_{n=1}$, we know optimaml $\theta$ and vice-versa.
Goal: solve for $\alpha$ instead of $\theta = \Phi^T \alpha$

Q: Can we solve for $\alpha$ more efficiently?

*note: 
$||\Phi^T\alpha||^2 = \alpha^T \Phi \Phi^T\alpha$*
$||a-b||^2 = (a-b)^T(a-b)$
*Positive semidefinite vector $a$ can be singular $\rightarrow$ no inverse*

**Prop of the matrix $K$**
* $(K)_{ij} = (K)_{ji}$ $\rightarrow$ symmetric matrix
* $K$ is positive semidefinite

