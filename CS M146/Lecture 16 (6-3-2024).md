# Neural Network
Goal: To learn the weights

**Transfer function for the neurons**
- Sigmoid: $h(z) = \frac{1}{1+e^{-z}}$
- tanh: $h(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$
- piecewise linear: h(z) = max(0, z) (ReLu)

**Forward Pass**
- Working forward through the network, the input function of each unit is applied to compute the input value
	- usually the weighted sum of the activation on the links
- activation function: sigmoid, tanh, ReLu
- The activation function transforms this input function into a final value

Optimization
$$\underset{w}{min} \sum^N_{i=1} l (h_w(x_i), y_i) $$
Goal: Optimize the unknown weights for a specified loss function.
*Done through gradient descent*
$\underline{\text{The optimization problem could be non-convex. }}$
Gradient descent cannot give guarantees of finding the optimal solution.

**Backward Pass**
To optimize functions, we compute derivatives in backward pass from right to left (backpropagation)
Goal: $\frac{\partial{f}}{\partial{x}}, \frac{\partial{f}}{\partial{y}}, \frac{\partial{f}}{\partial{z}}$

Activations $a_t$: inputs to the sigmoid function $\sigma(a)$.
$a_t[i] = \sum^{k_{t-1}}_{j=1} (W_{t-1})_{i,j} d_{t-1}[j]$
$a_t = W_{t-1}d_{t-1}$ is the activation input
decision outputs: $d_t = \sigma (a_t)$





