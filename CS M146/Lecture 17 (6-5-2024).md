Training a neural network
1. Randomly initialize problems

Deep Neural Networks
- num of hidden layers >> 1

Regularizing Deep Neural Nets
- Explicit regularization adds regularization to the cost function.
- DNNs incorporate regularization in implicit ways
- Dropout
- Momentum: variation on gradient descent

Dropout

Batch Normalization
Basic idea: you are changing your activation to a'.
$\mu = \frac{1}{m} \sum^m _{i=1} a_i$, $\sigma = \sqrt {\frac{1}{m} \sum^m_{i=1} (a_i-\mu)^2}$   
$$a' = \frac{a - \mu} {\sigma + \epsilon}, \text{ }a''=\gamma a_i' + \beta$$
Can you use modified gradient descent (1st order method)?

**Momentum**
$\beta$ controls the extent of smoothing
Idea: use momentum for smoothing gradients of mini-batch gradient descent
Smoothed gradient: 
$$
\begin{align}
V_t =& \beta V_{t-1} + (1-\beta)\nabla_\theta J(\theta_t)\\
\theta_{t+1} =& \theta_t - \alpha V_t
\end{align}
$$
-> You find a smoothed gradient and use that gradient to update the weight also.
momentum improves convergence rate and better generalization/test performance.



