Recap
* Use principle of composition of kernels to show valid kernel.
* Or show a counterexample to say k is not a valid kernel.
# SVM cont
Margin$(w,b)$ = $\underset{n}{min} \frac {y_n[w^T\phi(x_n)+b]}{||w||_2}$ 
*Scaling $(w,b)$ does not change the solution since it cancels out (Same margin)*
Using this, we can change the optimization problem to:
$$max \{\frac{1}{||w||_2} \underset{n}{min}\text{ }(y_n[w^T\phi (x_n) +b])\} = max\frac{1}{||w||_2}$$
Relaxing the statement:
$max\frac{1}{||w||_2}$ such that $\underset{n}{min}\text{ }(y_n[w^T\phi (x_n) +b]) = 1$ is equivalent to saying
$max\frac{1}{||w||_2}$ such that $\underset{n}{min}\text{ }(y_n[w^T\phi (x_n) +b]) \geq 1$.

Optimization problem (Convex program)
$$max (\frac{1}{||w||_2}) = min(||w||_2)$$
**Implication: Maximum margin classifier can be solved efficiently using a convex program for linearly separable data**

However, there may not be any feasible $(w,b)$ which works for all $\{(x_i,y_i)\}^N_{i=1}$ (not lineraly separable). We introduce slack variable to modify our constraints to account for non-separability.
$$y_n[w^T\phi (x_n) +b]\geq 1 - s_n, s_n\geq 0$$
To penalize $s_n$ growing too large, we control the size by incoporating them into our optimization problem: (Soft margin SVM)
$$\underset{w,b,s}{min} \frac{1}{2} ||w||^2_2 +C \sum_n s_n \text{ (C is a hyperparameter)}$$
$||w||^2_2$ is a convex and the summation is linear. Convex + Linear is still convex. Therefore, the whole expression is convex.

Q: What loss function is soft margin SVM solving for ?
A: Hinge loss

**Hinge Loss**
$\ell(y,a(x)) = \begin{cases}0 & \text{ if }ya(x) \geq 1 \\ 1-ya(x) & otherwise \end{cases}$ 
$a(x) = w^T\phi(x)+b$ 