**Decision Tree**
Given features, sequentially make decision leading you to final labels.

**ID3 Algorithm**
use the most informative features (features that reduce the uncertainty the most)

Greedy can be used to build the tree
*feature can have a real value but has to be given a range (threshold)*
choose by search to maximize informational gain
number of thresholds is hyperparameter

How far do you keep splitting?
- How do we choose hyperparameters

Cross-validation
- divide data into three categories: training, validation, and test data.
- Using training data, create a tree with particular choice of hyperparameter.
- validate the tree with validation data.
- use test data only after the final model is chosen.

# Nearest Neighbor Classification (NNC)

**Inductive Bias**
close-by points have same label
test points inherit the label of nearest neighbor in dataset
*Note:
No explicit model developed from dataset in contrast to previous methods
Implicit models do exist $\rightarrow$ inherit labels from close by point in dataset
$\rightarrow$ dataset is the model
No Explicit training* 

**Principle of nearest neighbor**
order the set such that $x_1$ (the first element) has the smallest distance between the new point and the data point.
Then, the new point inherits the label $y_1$.

*This principle can be applied to any distance measure, such as euclidean and manhattan.*

What if a feature is not real-valued ?
use one-hot encoding which translates a categorical feature with $|v|$ into a vector of size $|v|$.
Ex)
Given the color white, blue, and pink, 
We have [isitWhite, isitBlue, isitPink] and this turns into
$$
White = 
\begin{bmatrix}
1\\
0\\
0
\end{bmatrix},
Blue = \begin{bmatrix} 0\\ 1\\ 0\\ \end{bmatrix},
Pink = \begin{bmatrix} 0\\ 0\\ 1\\ \end{bmatrix}
$$
**Hyper Parameters**
- Feature choice
- Distance measure
- K

The performance is measured by cross validation
- Divide the data set into training and validation data
- Validate throughout the dataset and choose the final model.
* Evaluate the final model on test dataset.

**Advantages**
- No explicit model required as the dataset is the model itself.
**Disadvantages**
- Computationally intensive as you have to go through the whole set to identify the nearest neightbor
- Non-parametric: training data is always needed for classification.
- Choosing the right distance measure can be challenging.



